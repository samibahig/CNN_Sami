{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVtne/6ISVbsJYoqyZQvuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/CNN_Sami/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM6sjp5mRVHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on Wed Feb 12 18:28:35 2020\n",
        "\n",
        "@author: samib\n",
        "\"\"\"\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import gzip\n",
        "import tqdm\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "frame_log = tqdm.tqdm(total=0, position=4, bar_format='{desc}')\n",
        "\n",
        "def one_hot(y, n_classes=10):\n",
        "    return np.eye(n_classes)[y]\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    data_file = gzip.open(\"mnist.pkl.gz\", \"rb\")\n",
        "    train_data, val_data, test_data = pickle.load(data_file, encoding=\"latin1\")\n",
        "    data_file.close()\n",
        "\n",
        "    train_inputs = [np.reshape(x, (784, 1)) for x in train_data[0]]\n",
        "    train_results = [one_hot(y, 10) for y in train_data[1]]\n",
        "    train_data = np.array(train_inputs).reshape(-1, 784), np.array(train_results).reshape(-1, 10)\n",
        "\n",
        "    val_inputs = [np.reshape(x, (784, 1)) for x in val_data[0]]\n",
        "    val_results = [one_hot(y, 10) for y in val_data[1]]\n",
        "    val_data = np.array(val_inputs).reshape(-1, 784), np.array(val_results).reshape(-1, 10)\n",
        "\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
        "    test_data = list(zip(test_inputs, test_data[1]))\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "#train_data_, val_data_, test_data_ = load_mnist()\n",
        "\n",
        "class NN(object):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(784, 256),\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=64,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 data=None\n",
        "                 ):\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_hidden = len(hidden_dims)\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.init_method = 'Glorot'\n",
        "        self.seed = seed\n",
        "        self.activation_str = activation\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
        "\n",
        "        if data is None:\n",
        "            # for testing, do NOT remove or modify\n",
        "            self.train, self.valid, self.test = (\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400)))\n",
        "            )\n",
        "        else:\n",
        "            self.train, self.valid, self.test = data\n",
        "\n",
        "    def initialize_weights(self, dims):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
        "        for layer_n in range(1, self.n_hidden + 2):\n",
        "            if(self.init_method == \"Glorot\"):\n",
        "                d = np.sqrt(1 / all_dims[layer_n - 1])\n",
        "                W = np.random.uniform(-d, d, (all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "                self.weights[f\"W{layer_n}\"] = W\n",
        "                \n",
        "            elif(self.init_method == \"Normal\"):\n",
        "                self.weights[f\"W{layer_n}\"] = np.random.normal(0,1,size = (all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "            \n",
        "                    \n",
        "    def relu(self, x, grad=False):\n",
        "        if grad:\n",
        "            return x > 0\n",
        "            pass\n",
        "        return np.maximum(x, 0, x)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def sigmoid(self, x, grad=False):\n",
        "        if grad:\n",
        "            s = 1 / (1 + np.exp(-x))\n",
        "            return s * (s - 1)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        return 0\n",
        "\n",
        "    def tanh(self, x, grad=False):\n",
        "        if grad:\n",
        "            return 1.0 - np.tanh(x) ** 2\n",
        "        return np.tanh(x)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def activation(self, x, grad=False):\n",
        "        rst = 0\n",
        "        if self.activation_str == \"relu\":\n",
        "            rst=self.relu(x, grad)\n",
        "        elif self.activation_str == \"sigmoid\":\n",
        "            rst=self.sigmoid(x, grad)\n",
        "        elif self.activation_str == \"tanh\":\n",
        "            rst=self.tanh(x, grad)\n",
        "        else:\n",
        "            raise Exception(\"invalid\")\n",
        "        return rst\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
        "        exps = np.exp(x)\n",
        "        sum_exp= np.sum(exps, axis=0)\n",
        "        return exps / sum_exp\n",
        "\n",
        "    def forward(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        # cache is a dictionnary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
        "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "        # WRITE CODE HERE\n",
        "        Z = x.T\n",
        "        cache[f\"Z{str(0)}\"] = Z\n",
        "        for layer_n in range(1, self.n_hidden + 1):\n",
        "            W = self.weights[\"W\" + str(layer_n)]\n",
        "            b = self.weights[\"b\" + str(layer_n)]\n",
        "            # print('Shape W' + str(np.transpose(W).shape))\n",
        "            # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "            A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "            Z = self.activation(A)\n",
        "            cache[f\"A{str(layer_n)}\"] = A\n",
        "            cache[f\"Z{str(layer_n)}\"] = Z\n",
        "\n",
        "        #Apply softmax\n",
        "        W = self.weights[\"W\" + str(self.n_hidden + 1)]\n",
        "        b = self.weights[\"b\" + str(self.n_hidden + 1)]\n",
        "        # print('Shape W' + str(np.transpose(W).shape))\n",
        "        # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "        A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "        Z = self.softmax(A)\n",
        "        cache[f\"A{str(self.n_hidden + 1)}\"] = A\n",
        "        cache[f\"Z{str(self.n_hidden + 1)}\"] = Z\n",
        "\n",
        "        return cache\n",
        "\n",
        "    def backward(self, cache, labels):\n",
        "        # print(\"cache \" + str(cache.keys()))\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "        n = len(labels)\n",
        "        grads = {}\n",
        "        dA = output - labels.T\n",
        "        prev_activ = cache[f\"Z\" + str(self.n_hidden)]\n",
        "        dW = (1. / n) * np.dot(dA, prev_activ.T)\n",
        "        db = (1. / n) * np.sum(dA, axis=1, keepdims=True)\n",
        "\n",
        "        dZ = np.dot(self.weights[\"W\" + str(self.n_hidden + 1)], dA)\n",
        "        grads[\"dW\" + str(self.n_hidden + 1)] = dW\n",
        "        grads[\"db\" + str(self.n_hidden + 1)] = db\n",
        "\n",
        "        for layer_n in range(self.n_hidden, 0, -1):\n",
        "            grad_prev_activ = self.activation(cache[\"A\" + str(layer_n)],True)\n",
        "            dA = dZ * grad_prev_activ\n",
        "            prev_activ_v = cache[\"Z\" + str(layer_n - 1)]\n",
        "            dW = 1./n * np.dot(dA, prev_activ_v.T)\n",
        "            db = 1./n * np.sum(dA, axis=1, keepdims=True)\n",
        "            if layer_n >= 1:\n",
        "                W_prev_layer = self.weights[\"W\" + str(layer_n)]\n",
        "                dZ = np.dot(W_prev_layer, dA)\n",
        "            grads[\"dW\" + str(layer_n)] = dW\n",
        "            grads[\"db\" + str(layer_n)] = db\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        for layer in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            W = self.weights[\"W\" + str(layer)]\n",
        "            b = self.weights[\"b\" + str(layer)]\n",
        "\n",
        "            W = W - self.lr * grads[\"dW\" + str(layer)].T\n",
        "            b = b - self.lr * grads[\"db\" + str(layer)].T\n",
        "\n",
        "            self.weights.update({\"W\" + str(layer): W, \"b\" + str(layer): b})\n",
        "\n",
        "    # def one_hot(self, y, n_classes=None):\n",
        "    #     n_classes = n_classes or self.n_classes\n",
        "    #     return np.eye(n_classes)[y]\n",
        "\n",
        "    def loss(self, prediction, labels):\n",
        "        prediction = np.multiply(prediction, labels)\n",
        "        precision = np.max(prediction, axis=1)\n",
        "        log_precision = np.log(precision, out=np.zeros_like(precision), where=(precision != 0))\n",
        "        log_err = np.multiply(log_precision, -1)\n",
        "        err = np.mean(log_err)\n",
        "        return err\n",
        "\n",
        "    def compute_loss_and_accuracy(self, X, y):\n",
        "        one_y = y\n",
        "        y = np.argmax(y, axis=1)  # Change y to integers\n",
        "        cache = self.forward(X)\n",
        "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=0)\n",
        "        accuracy = np.mean(y == predictions)\n",
        "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y.T)\n",
        "        return loss, accuracy, predictions\n",
        "\n",
        "    def train_loop(self, n_epochs):\n",
        "        X_train, y_train = self.train\n",
        "        y_onehot = y_train\n",
        "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
        "        self.initialize_weights(dims)\n",
        "\n",
        "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            for batch in range(n_batches):\n",
        "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                # WRITE CODE HERE\n",
        "                # Forward\n",
        "                cache = self.forward(minibatchX)\n",
        "                # Backward\n",
        "                grads = self.backward(cache, minibatchY)\n",
        "                # Update\n",
        "                self.update(grads)\n",
        "\n",
        "            X_train, y_train = self.train\n",
        "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
        "            X_valid, y_valid = self.valid\n",
        "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
        "\n",
        "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
        "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
        "            self.train_logs['train_loss'].append(train_loss)\n",
        "            self.train_logs['validation_loss'].append(valid_loss)\n",
        "\n",
        "        print(\"The avg training loss for the {} method as init_method is {}\".format(self.init_method,np.mean( self.train_logs['train_accuracy'])))\n",
        "        return self.train_logs\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test, y_test = self.test\n",
        "        test_loss, test_accuracy, _ = self.compute_loss_and_accuracy(X_test, y_test)\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "class NeuralMLP(NN):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(784, 256),\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=64,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 data=None,\n",
        "                 init_method=\"zero\"\n",
        "                 ):\n",
        "        NN.__init__(self,hidden_dims,epsilon,lr,batch_size,seed,activation,data)\n",
        "        self.init_method = init_method\n",
        "\n",
        "\n",
        "    def initialize_weights(self, dims):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        nb_param = 0\n",
        "        # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
        "\n",
        "        if self.init_method == \"glorot\":\n",
        "            for layer_n in range(1, self.n_hidden + 2):\n",
        "                d = np.sqrt(6 / all_dims[layer_n - 1])\n",
        "                W = np.random.uniform(-d, d, (all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "                b = np.zeros((1, all_dims[layer_n]))\n",
        "                self.weights[f\"W{layer_n}\"] = W\n",
        "                self.weights[f\"b{layer_n}\"] = b\n",
        "                nb_param = nb_param + W.size + b.size\n",
        "\n",
        "        if self.init_method == 'normal':\n",
        "            for layer_n in range(1, self.n_hidden + 2):\n",
        "                W = np.random.normal(loc = 0.0, scale = 1.0,size=(all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "                b = np.zeros((1, all_dims[layer_n]))\n",
        "                self.weights[f\"W{layer_n}\"] = W\n",
        "                self.weights[f\"b{layer_n}\"] = b\n",
        "                nb_param = nb_param + W.size + b.size\n",
        "\n",
        "\n",
        "        if self.init_method == 'zero':\n",
        "            for layer_n in range(1, self.n_hidden + 2):\n",
        "                W = np.zeros((all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "                b = np.zeros((1, all_dims[layer_n]))\n",
        "                self.weights[f\"W{layer_n}\"] = W\n",
        "                self.weights[f\"b{layer_n}\"] = b\n",
        "                nb_param = nb_param + W.size + b.size\n",
        "\n",
        "        print(\"The number of parameter is {}\".format(nb_param))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = load_mnist()\n",
        "\n",
        "nn = NeuralMLP(hidden_dims=(300,400), data=data, init_method=\"zero\", activation=\"relu\",lr=0.0007)\n",
        "print(nn.train_loop(10))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}